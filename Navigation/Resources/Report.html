<!doctype html><html><head><meta charset="utf-8">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/2.10.0/github-markdown.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js">
<link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
<link rel="stylesheet" href="https://gitcdn.xyz/repo/goessner/mdmath/master/css/texmath.css">
<link rel="stylesheet" href="https://gitcdn.xyz/repo/goessner/mdmath/master/css/vscode-texmath.css">

</head><body class="markdown-body">
<h1 id="project-1-navigation----omit-in-toc" data-line="0" class="code-line">Project 1: Navigation <!-- omit in toc --></h1>
<ul>
<li data-line="6" class="code-line"><a href="#introduction">Introduction</a></li>
<li data-line="7" class="code-line"><a href="#description">Description</a></li>
<li data-line="8" class="code-line"><a href="#results">Results</a>
<ul>
<li data-line="9" class="code-line"><a href="#building-the-network-architecture">Building the Network Architecture</a></li>
<li data-line="10" class="code-line"><a href="#training-the-model">Training the Model</a></li>
</ul>
</li>
<li data-line="11" class="code-line"><a href="#moving-forward">Moving Forward</a></li>
</ul>
<h1 id="introduction" data-line="13" class="code-line">Introduction</h1>
<p data-line="15" class="code-line">This project explores solution to solving a Unity virtual environement using Deep Reinforcement Learning, specifically a Deep Q-Network. The virtual environment is a space (large room) filled with two kinds of objects that needs to be collected or avoided. In this instance, the objective is to collect yellow bananas and avoid blue ones.</p>
<p data-line="17" class="code-line">Our agent will be trained based on the following parameters:</p>
<p data-line="19" class="code-line"><img src="https://user-images.githubusercontent.com/10624937/42135619-d90f2f28-7d12-11e8-8823-82b970a54d7e.gif" alt="Trained Agent" title="Trained Agent" class="loading" id="image-hash-7e250f87e2988b379948b25d1d7f1c13576162b2918b7d2f1f67d790556d547c"></p>
<p data-line="21" class="code-line">A reward of +1 is provided for collecting a yellow banana, and a reward of -1 is provided for collecting a blue banana.  Thus, the goal of your agent is to collect as many yellow bananas as possible while avoiding blue bananas.</p>
<p data-line="23" class="code-line">The state space has 37 dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  Given this information, the agent has to learn how to best select actions.  Four discrete actions are available, corresponding to:</p>
<ul>
<li data-line="24" class="code-line"><strong><code>0</code></strong> - move forward.</li>
<li data-line="25" class="code-line"><strong><code>1</code></strong> - move backward.</li>
<li data-line="26" class="code-line"><strong><code>2</code></strong> - turn left.</li>
<li data-line="27" class="code-line"><strong><code>3</code></strong> - turn right.</li>
</ul>
<p data-line="29" class="code-line">The task is episodic, and in order to solve the environment, your agent must get an average score of +13 over 100 consecutive episodes.</p>
<h1 id="description" data-line="31" class="code-line">Description</h1>
<p data-line="33" class="code-line">A <strong>Deep Q Network</strong> agent is a <strong>deep neural network that acts as a function approximator</strong>. You pass in inputs (state space), and it produces a vector of action values, with the max value indicating the action to take. As a reinforcement signal, it is fed back the change in score at each time step. At first, the neural network's weights and biases are initialized with random numbers but change over time as it begins to associate states with appropriate actions and learns to navigate the virtual environement.</p>
<p data-line="35" class="code-line"><strong>The Deep Q network is designed to produce a Q value for every possible action in a single forward pass.</strong> However, such a network can become unstable due to the <strong>high correlation between actions and states</strong>. Let's discuss two methodologies to help with the issue:</p>
<ul>
<li data-line="37" class="code-line">Experience Replay</li>
<li data-line="38" class="code-line">Fixed Q Targets</li>
</ul>
<p data-line="40" class="code-line">In <strong>Experience Replay</strong>, instead of discarding the <strong>state-action-reward-next_state tuple</strong> at each time step, a <strong>replay buffer</strong> <em>stores</em> those tuples (in our case, <strong>100000</strong> tuples). We store each experienced tuple in this buffer as we are interacting with the environment and then sample a small batch of tuples from it in order to learn. As a result, we are able to learn from individual tuples multiple times.</p>
<p data-line="42" class="code-line"><img src="/home/adebraine/Documents/Personal%20Projects/Git%20Repos/RLND-Projects/Navigation/Resources/1.png" alt="" class="loading" id="image-hash-25bf14e3360a3731ed5181492f814eb3f2a7e24df14f2370e607eac463c10482"></p>
<p data-line="45" class="code-line">In <strong>Fixed Q Targets</strong>, we try to solve the correlation between the <strong>target</strong> and the <strong>parameters</strong> we are changing. When we train our network, the objective is to get closer to our target by updating our network's weights, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi><mo>=</mo><mi>R</mi><mo>+</mo><mi>γ</mi><msub><mi>max</mi><mo>⁡</mo><mi>a</mi></msub><mover accent="true"><mi>q</mi><mo>^</mo></mover><mo>(</mo><msup><mi>S</mi><mo mathvariant="normal">′</mo></msup><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><mi mathvariant="bold">w</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">target=R + \gamma \max_{a} \hat{q} (S&#x27;, a, \mathbf{w})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.80952em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">e</span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.001892em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop"><span class="mop">max</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">a</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;">^</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">w</span></span><span class="mclose">)</span></span></span></span>. However, we notice that our target is also dependent on our weights which leads to a highly variable target value. In order to stabilize our target, we build a second identical network whose weights are updated less often. In other words, the two networks are updated using the same equation but one updates less often. The final equation becomes:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">Δ</mi><mi mathvariant="bold">w</mi><mo>=</mo><mi>α</mi><mo fence="false">(</mo><mi>R</mi><mo>+</mo><mi>γ</mi><munder><mi>max</mi><mo>⁡</mo><mi>a</mi></munder><mover accent="true"><mi>q</mi><mo>^</mo></mover><mo>(</mo><msup><mi>S</mi><mo mathvariant="normal">′</mo></msup><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><msup><mi mathvariant="bold">w</mi><mo>−</mo></msup><mo>)</mo><mo>−</mo><mover accent="true"><mi>q</mi><mo>^</mo></mover><mo>(</mo><mi>S</mi><mo separator="true">,</mo><mi>A</mi><mo separator="true">,</mo><mi mathvariant="bold">w</mi><mo>)</mo><mo fence="false">)</mo><msub><mi mathvariant="normal">∇</mi><mi mathvariant="bold">w</mi></msub><mover accent="true"><mi>q</mi><mo>^</mo></mover><mo>(</mo><mi>S</mi><mo separator="true">,</mo><mi>A</mi><mo separator="true">,</mo><mi mathvariant="bold">w</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">
\Delta \mathbf{w} = \alpha \Big (R + \gamma \max_{a} \hat{q} (S&#x27;, a, \mathbf{w}^-) - \hat{q} (S, A, \mathbf{w}) \Big ) \nabla_{\mathbf{w}} \hat{q} (S, A, \mathbf{w})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">Δ</span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">w</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.80002em;vertical-align:-0.65002em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="mord"><span class="delimsizing size2">(</span></span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.521331em;vertical-align:-0.7em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.43056em;"><span style="top:-2.1em;margin-left:0em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">a</span></span></span></span><span style="top:-2.7em;"><span class="pstrut" style="height:2.7em;"></span><span><span class="mop">max</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;">^</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">w</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.821331em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">−</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.80002em;vertical-align:-0.65002em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;">^</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">A</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">w</span></span><span class="mclose">)</span><span class="mord"><span class="delimsizing size2">)</span></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.161108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathbf mtight" style="margin-right:0.01597em;">w</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;">^</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">A</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">w</span></span><span class="mclose">)</span></span></span></span></span></p>
<p data-line="50" class="code-line">with:</p>
<ul>
<li data-line="51" class="code-line"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="bold">w</mi></mrow><annotation encoding="application/x-tex">\mathbf{w}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.44444em;vertical-align:0em;"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">w</span></span></span></span></span> : The main network's weights</li>
<li data-line="52" class="code-line"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi mathvariant="bold">w</mi><mo>−</mo></msup></mrow><annotation encoding="application/x-tex">\mathbf{w^-}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.771331em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.771331em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">−</span></span></span></span></span></span></span></span></span></span></span></span> : The target network's weights</li>
<li data-line="53" class="code-line"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi mathvariant="bold">w</mi><mo>−</mo></msup><mo>←</mo><mi mathvariant="bold">w</mi></mrow><annotation encoding="application/x-tex">\mathbf{w^-} \leftarrow \mathbf{w}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.771331em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.771331em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">−</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">←</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.44444em;vertical-align:0em;"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">w</span></span></span></span></span> : at every time step (in our case, every <strong>4</strong> time steps)</li>
</ul>
<p data-line="55" class="code-line">Finally, we train our network using a <strong><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">ϵ</span></span></span></span>-greedy policy</strong>, meaning that the action is chosen based on a probability <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">ϵ</span></span></span></span> between a random action and an action decided by our network. At first we favor random actions in order to explore possibilities then as time goes on, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">ϵ</span></span></span></span> decays to then favor our network's actions.</p>
<h1 id="results" data-line="57" class="code-line">Results</h1>
<h2 id="building-the-network-architecture" data-line="59" class="code-line">Building the Network Architecture</h2>
<p data-line="60" class="code-line">Importing the Deep Q-Network Agent and building the architecture based on specific hyperparameters</p>
<ul>
<li data-line="62" class="code-line">3 hidden layers:
<ul>
<li data-line="63" class="code-line">(Number of States=37, 512)</li>
<li data-line="64" class="code-line">(512, 256)</li>
<li data-line="65" class="code-line">(256, 128)</li>
<li data-line="66" class="code-line">(128, Number of Actions=4)</li>
</ul>
</li>
<li data-line="67" class="code-line">A batch size of 32</li>
<li data-line="68" class="code-line">A learning rate of 0.0005</li>
<li data-line="69" class="code-line">buffer size: 1e5</li>
<li data-line="70" class="code-line">target network updated every 4 time steps</li>
</ul>
<pre><code data-line="72" class="code-line language-python"><div>device = torch.device(<span class="hljs-string">"cuda:0"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>)
agent = DQN_Agent(state_size=state_size,
                  action_size=action_size,
                  hidden_layers=[<span class="hljs-number">512</span>,<span class="hljs-number">256</span>,<span class="hljs-number">128</span>],
                  buffer_size=int(<span class="hljs-number">1e5</span>),
                  batch_size=<span class="hljs-number">32</span>,
                  gamma=<span class="hljs-number">0.99</span>,
                  tau=<span class="hljs-number">1e-3</span>,
                  lr=<span class="hljs-number">5e-4</span>,
                  update_every=<span class="hljs-number">4</span>,
                  drop_p=<span class="hljs-number">0.0</span>,
                  device=device,
                  seed=<span class="hljs-number">0</span>)
</div></code></pre>
<h2 id="training-the-model" data-line="88" class="code-line">Training the Model</h2>
<p data-line="89" class="code-line">Training the agent based on the following hyperparameters:</p>
<ul>
<li data-line="90" class="code-line"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">ϵ</span></span></span></span> decay rate of 0.95
<ul>
<li data-line="91" class="code-line">Tested multiple values (0.995, 0.999, 0.99, 0.95). Iterating through these values improved the training speed the most.</li>
</ul>
</li>
</ul>
<pre><code data-line="94" class="code-line language-python"><div>scores = train(agent=agent,
               env=env,
               n_episodes=<span class="hljs-number">2000</span>,
               max_t=<span class="hljs-number">1000</span>,
               eps_start=<span class="hljs-number">1.0</span>,
               eps_end=<span class="hljs-number">0.005</span>,
               eps_decay=<span class="hljs-number">0.95</span>)
</div></code></pre>
<pre data-line="104" class="code-line"><code>Episode 100	Average Score: 6.11
Episode 200	Average Score: 11.08
Episode 300	Average Score: 11.69
Episode 371	Average Score: 13.08
Environment solved in 271 episodes!	Average Score: 13.08
</code></pre>
<p data-line="110" class="code-line"><img src="vscode-resource:/home/adebraine/Documents/Personal%20Projects/Git%20Repos/RLND-Projects/Navigation/Resources/output_14_0.png" alt="png" class="loading" id="image-hash-eb4ffb3ea7532861c79076364a53ab8b6f5d42f945ac6d640b68ab34456e7bc8"></p>
<h1 id="moving-forward" data-line="113" class="code-line">Moving Forward</h1>
<p data-line="114" class="code-line">We would like to explore a different approach, only using pixels (the video feed from the environment) as inputs instead of a well manufactured state space. Additionally, we would like to explore more complex methodologies:</p>
<ul>
<li data-line="115" class="code-line">Double DQN</li>
<li data-line="116" class="code-line">Prioritized Experience Replay</li>
<li data-line="117" class="code-line">Dueling DQN</li>
<li data-line="118" class="code-line">Rainbow</li>
</ul>

</body></html>